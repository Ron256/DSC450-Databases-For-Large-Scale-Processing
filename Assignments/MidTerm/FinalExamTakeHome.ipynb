{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets\n",
      "Processed 5000 tweets\n",
      "Processed 10000 tweets\n",
      "Processed 15000 tweets\n",
      "Processed 20000 tweets\n",
      "Processed 25000 tweets\n",
      "Processed 30000 tweets\n",
      "Processed 35000 tweets\n",
      "Processed 40000 tweets\n",
      "Processed 45000 tweets\n",
      "Processed 50000 tweets\n",
      "Processed 55000 tweets\n",
      "Processed 60000 tweets\n",
      "Processed 65000 tweets\n",
      "Processed 70000 tweets\n",
      "Processed 75000 tweets\n",
      "Processed 80000 tweets\n",
      "Processed 85000 tweets\n",
      "Processed 90000 tweets\n",
      "Processed 95000 tweets\n",
      "Processed 100000 tweets\n",
      "Processed 105000 tweets\n",
      "Processed 110000 tweets\n",
      "Processed 115000 tweets\n",
      "Processed 120000 tweets\n",
      "Processed 125000 tweets\n",
      "Processed 130000 tweets\n",
      "Processed 135000 tweets\n",
      "Processed 140000 tweets\n",
      "Processed 145000 tweets\n",
      "Processed 150000 tweets\n",
      "Processed 155000 tweets\n",
      "Processed 160000 tweets\n",
      "Processed 165000 tweets\n",
      "Processed 170000 tweets\n",
      "Processed 175000 tweets\n",
      "Processed 180000 tweets\n",
      "Processed 185000 tweets\n",
      "Processed 190000 tweets\n",
      "Processed 195000 tweets\n",
      "Processed 200000 tweets\n",
      "Processed 205000 tweets\n",
      "Processed 210000 tweets\n",
      "Processed 215000 tweets\n",
      "Processed 220000 tweets\n",
      "Processed 225000 tweets\n",
      "Processed 230000 tweets\n",
      "Processed 235000 tweets\n",
      "Processed 240000 tweets\n",
      "Processed 245000 tweets\n",
      "The processing of the tweets data took 447.0908958911896 seconds\n",
      "The number of operations per second is 1118.3408219560058 seconds\n"
     ]
    }
   ],
   "source": [
    "# Author: Ronaldlee Ejalu\r\n",
    "# Course DSC 450\r\n",
    "# 1a\r\n",
    "\"\"\"\r\n",
    "a.Use python to download tweets from the web and save to a local text file (not into a database yet, just to a text file). \r\n",
    "This is as simple as it sounds, all you need is a for-loop that reads lines and writes them into a file, just don’t forget to add ‘\\n’ at the end so they are, in fact, on separate lines.\r\n",
    "NOTE: Do not call read() or readlines(). \r\n",
    "That command will attempt to read the entire file which is too much data. Clicking on the link in the browser would cause the same prob\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import time\r\n",
    "\r\n",
    "os.chdir('C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome')\r\n",
    "\r\n",
    "tweetdata = \"\"\"http://dbgroup.cdm.depaul.edu/DSC450/OneDayOfTweets.txt\"\"\"\r\n",
    "startTime = time.time()                                                     # start time of processing the file in web\r\n",
    "\r\n",
    "webFD = urllib.request.urlopen(tweetdata)\r\n",
    "# csvf = open('OneDayOfTweets.csv', 'w', newline = '\\n', encoding = 'utf-8')\r\n",
    "csvf = open('OneDayOfTweets.csv', 'wb')\r\n",
    "\r\n",
    "for i in range(500000):\r\n",
    "    if i % 5000 == 0: # Print a message every 500th tweet read\r\n",
    "        print (\"Processed \" + str(i) + \" tweets\")\r\n",
    "    try:\r\n",
    "        itemResponse = webFD.readline()                                     # read one line at a time\r\n",
    "        # strItemResponse = itemResponse.decode('utf-8')                      # decode the line that comes back from the web into a string.\r\n",
    "        csvf.write(itemResponse)\r\n",
    "    \r\n",
    "    except Exception:\r\n",
    "        continue\r\n",
    "    \r\n",
    "csvf.close()                                                                 # close the file\r\n",
    "# csve.close()                                                                 # close the error file\r\n",
    "endTime = time.time()                                                        # end time of processing of writing the tweets data to a file. \r\n",
    "print('The processing of the tweets data took %s seconds' %(endTime-startTime))\r\n",
    "print('The number of operations per second is %s seconds' %(500000/(endTime-startTime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets\n",
      "Processed 5000 tweets\n",
      "Processed 10000 tweets\n",
      "Processed 15000 tweets\n",
      "Processed 20000 tweets\n",
      "Processed 25000 tweets\n",
      "Processed 30000 tweets\n",
      "Processed 35000 tweets\n",
      "Processed 40000 tweets\n",
      "Processed 45000 tweets\n",
      "Processed 50000 tweets\n",
      "Processed 55000 tweets\n",
      "Processed 60000 tweets\n",
      "Processed 65000 tweets\n",
      "Processed 70000 tweets\n",
      "Processed 75000 tweets\n",
      "Processed 80000 tweets\n",
      "Processed 85000 tweets\n",
      "Processed 90000 tweets\n",
      "Processed 95000 tweets\n",
      "Processed 100000 tweets\n",
      "Processed 105000 tweets\n",
      "Processed 110000 tweets\n",
      "Processed 115000 tweets\n",
      "Processed 120000 tweets\n",
      "Processed 125000 tweets\n",
      "Processed 130000 tweets\n",
      "Processed 135000 tweets\n",
      "Processed 140000 tweets\n",
      "Processed 145000 tweets\n",
      "Processed 150000 tweets\n",
      "Processed 155000 tweets\n",
      "Processed 160000 tweets\n",
      "Processed 165000 tweets\n",
      "Processed 170000 tweets\n",
      "Processed 175000 tweets\n",
      "Processed 180000 tweets\n",
      "Processed 185000 tweets\n",
      "Processed 190000 tweets\n",
      "Processed 195000 tweets\n",
      "Processed 200000 tweets\n",
      "Processed 205000 tweets\n",
      "Processed 210000 tweets\n",
      "Processed 215000 tweets\n",
      "Processed 220000 tweets\n",
      "Processed 225000 tweets\n",
      "Processed 230000 tweets\n",
      "Processed 235000 tweets\n",
      "Processed 240000 tweets\n",
      "Processed 245000 tweets\n",
      "The length of the longest string in the file for the in_reply_to_user_id column is 12\n",
      "The length of the longest string in the file for the in_reply_to_screen_name column is 15\n",
      "The length of the longest string in the file for the ScreenName column is 15\n",
      "The processing of the tweets data took 2109.93319773674 seconds\n"
     ]
    }
   ],
   "source": [
    "# Author Ronaldlee Ejalu\r\n",
    "# Course DSC 450\r\n",
    "# 1b\r\n",
    "import pandas as pd\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import time\r\n",
    "import json\r\n",
    "# 1b\r\n",
    "\r\n",
    "fileName = 'C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome/OneDayOfTweets.csv'\r\n",
    "\r\n",
    "def extractLine():\r\n",
    "    \"\"\"Reading the file in chunks\"\"\"\r\n",
    "    with open(fileName, 'rb') as f:\r\n",
    "        for item in f:\r\n",
    "            yield item\r\n",
    "\r\n",
    "startTime = time.time()\r\n",
    "chunkSize = 500000\r\n",
    "generatedLines = extractLine()                              # invoke a helper extractLine\r\n",
    "screenNameDict = {}\r\n",
    "chunk = [i for i, j in zip(generatedLines, range(chunkSize))]\r\n",
    "\r\n",
    "inReplytoUserIdL = [] # hold in_reply_to_user_id id values\r\n",
    "\r\n",
    "inReplyToScreenNameL = [] # hold in_reply_to_screen_name values\r\n",
    "\r\n",
    "\r\n",
    "for i in range(500000):\r\n",
    "    if i % 5000 == 0: # Print a message every 500th tweet read\r\n",
    "        print ('Processed ' + str(i) + ' tweets')\r\n",
    "    fileDict = json.loads(chunk[i].decode('utf-8')) # using decode() and loads to convert each item to a dictionary\r\n",
    "    if fileDict['user']['screen_name'] not in screenNameDict.values():\r\n",
    "        screenNameDict[i] = fileDict['user']['screen_name']\r\n",
    "\r\n",
    "    if fileDict['in_reply_to_user_id'] not in inReplytoUserIdL:\r\n",
    "        inReplytoUserIdL.append(fileDict['in_reply_to_user_id'])\r\n",
    "\r\n",
    "    if fileDict['in_reply_to_screen_name'] not in inReplyToScreenNameL:\r\n",
    "        inReplyToScreenNameL.append(fileDict['in_reply_to_screen_name'])\r\n",
    "\r\n",
    "# print(screenNameDict)       # for debugging purposes\r\n",
    "\r\n",
    "screenNameDf = pd.DataFrame(screenNameDict.values(), columns=['ScreenName'])                    # generate the ScreenName DataFrame\r\n",
    "inReplytoUserIdDf = pd.DataFrame(inReplytoUserIdL, columns=['in_reply_to_user_id'])              # generate the inReplytoUserIdDf\r\n",
    "inReplyToScreenNameDf = pd.DataFrame(inReplyToScreenNameL, columns=['in_reply_to_screen_name'])\r\n",
    "\r\n",
    "# print(screenNameDf.head(10))   \r\n",
    "print('The length of the longest string in the file for the in_reply_to_user_id column is %s'%(inReplytoUserIdDf.in_reply_to_user_id.fillna(method='ffill').astype(str).str.len().max()))                        # use pandas.fillna() yo get rid of the NaNs and before deriving the min and max lengths trasnform each value into a string\r\n",
    "print('The length of the longest string in the file for the in_reply_to_screen_name column is %s'%(inReplyToScreenNameDf.in_reply_to_screen_name.fillna(method='ffill').astype(str).str.len().max()))\r\n",
    "print('The length of the longest string in the file for the ScreenName column is %s'%(screenNameDf.ScreenName.fillna(method='ffill').astype(str).str.len().max())) \r\n",
    "endTime = time.time()\r\n",
    "print('The processing of the tweets data took %s seconds' %(endTime-startTime))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets\n",
      "Processed 5000 tweets\n",
      "Processed 10000 tweets\n",
      "Processed 15000 tweets\n",
      "Processed 20000 tweets\n",
      "Processed 25000 tweets\n",
      "Processed 30000 tweets\n",
      "Processed 35000 tweets\n",
      "Processed 40000 tweets\n",
      "Processed 45000 tweets\n",
      "Processed 50000 tweets\n",
      "Processed 55000 tweets\n",
      "Processed 60000 tweets\n",
      "Processed 65000 tweets\n",
      "Processed 70000 tweets\n",
      "Processed 75000 tweets\n",
      "Processed 80000 tweets\n",
      "Processed 85000 tweets\n",
      "Processed 90000 tweets\n",
      "Processed 95000 tweets\n",
      "Processed 100000 tweets\n",
      "Processed 105000 tweets\n",
      "Processed 110000 tweets\n",
      "Processed 115000 tweets\n",
      "Processed 120000 tweets\n",
      "Processed 125000 tweets\n",
      "Processed 130000 tweets\n",
      "Processed 135000 tweets\n",
      "Processed 140000 tweets\n",
      "Processed 145000 tweets\n",
      "Processed 150000 tweets\n",
      "Processed 155000 tweets\n",
      "Processed 160000 tweets\n",
      "Processed 165000 tweets\n",
      "Processed 170000 tweets\n",
      "Processed 175000 tweets\n",
      "Processed 180000 tweets\n",
      "Processed 185000 tweets\n",
      "Processed 190000 tweets\n",
      "Processed 195000 tweets\n",
      "Processed 200000 tweets\n",
      "Processed 205000 tweets\n",
      "Processed 210000 tweets\n",
      "Processed 215000 tweets\n",
      "Processed 220000 tweets\n",
      "Processed 225000 tweets\n",
      "Processed 230000 tweets\n",
      "Processed 235000 tweets\n",
      "Processed 240000 tweets\n",
      "Processed 245000 tweets\n",
      "The number of records in the userTweets table are [(230593,)]\n",
      "The number of records in the Tweets table are [(250000,)]\n",
      "The number of records in the Geo table are [(5752,)]\n",
      "The processing of the tweets data took 582.7943189144135 seconds\n"
     ]
    }
   ],
   "source": [
    "# Author: Ronaldlee Ejalu\r\n",
    "# Course DSC 540\r\n",
    "# Assignment module 9\r\n",
    "# reading a collection of tweets data\r\n",
    "# extending the schema by adding Geo table \r\n",
    "# also, added a foreign key relationship between Geo and Tweets. \r\n",
    "# Part 1 c\r\n",
    "\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import re\r\n",
    "import sqlite3\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import time\r\n",
    "\r\n",
    "os.chdir('C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome')\r\n",
    "\r\n",
    "tweetdata = \"\"\"http://dbgroup.cdm.depaul.edu/DSC450/OneDayOfTweets.txt\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "createTbl1 = \"\"\"\r\n",
    "CREATE TABLE UserTweets \r\n",
    "(\r\n",
    "    Id VARCHAR2(100),\r\n",
    "    name VARCHAR2(100),\r\n",
    "    screen_name VARCHAR2(15),\r\n",
    "    description VARCHAR2(500),\r\n",
    "    friends_count NUMBER,\r\n",
    "    CONSTRAINT UserTweets_PK Primary Key(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl2 = \"\"\"\r\n",
    "CREATE TABLE Tweets \r\n",
    "(\tCREATED_AT DATE, \r\n",
    "\tID VARCHAR2(100), \r\n",
    "\tTEXT VARCHAR2(300), \r\n",
    "\tSOURCE VARCHAR2(100), \r\n",
    "\tIN_REPLY_TO_USER_ID VARCHAR2(12), \r\n",
    "\tIN_REPLY_TO_SCREEN_NAME VARCHAR2(15), \r\n",
    "\tIN_REPLY_TO_STATUS_ID VARCHAR2(100), \r\n",
    "\tRETWEET_COUNT NUMBER, \r\n",
    "\tCONTRIBUTORS VARCHAR2(100),\r\n",
    "    User_Id VARCHAR2(100),\r\n",
    "    GeoId VARCHAR2(1000),\r\n",
    "    CONSTRAINT TWEETS_FK1 FOREIGN KEY(User_Id) REFERENCES UserTweets(Id),\r\n",
    "    CONSTRAINT TWEETS_FK2 FOREIGN KEY(GeoId) REFERENCES Geo(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl3 = \"\"\"\r\n",
    "CREATE TABLE Geo\r\n",
    "(\r\n",
    "    Id VARCHAR2(1000),\r\n",
    "    Type VARCHAR2(50),\r\n",
    "    longitude NUMBER,\r\n",
    "    latitude NUMBER,\r\n",
    "    CONSTRAINT Geo_PK Primary Key (Id)\r\n",
    "); \r\n",
    "\"\"\"\r\n",
    "\r\n",
    "conn = sqlite3.connect('dsc450.db')                                                                        # open the connection\r\n",
    "cur = conn.cursor()                                                                                        # instantiate a cursor object\r\n",
    "\r\n",
    "# Drop the tables if they exist\r\n",
    "cur.execute('DROP TABLE IF EXISTS UserTweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Tweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Geo;')\r\n",
    "\r\n",
    "# execute the DDL to create the corresponding tables\r\n",
    "cur.execute(createTbl1)\r\n",
    "cur.execute(createTbl3)\r\n",
    "cur.execute(createTbl2)\r\n",
    "\r\n",
    "\r\n",
    "def transformExtraneousValues(tDictkey):\r\n",
    "    \"\"\"A function that takes a dictionary key and \r\n",
    "    checks if the value is null, an empty string or [] \r\n",
    "    and it replaces it with None otherwise it assigns \r\n",
    "    the actual value to a variable which is returned\r\n",
    "    \"\"\"\r\n",
    "    valuestr = ''\r\n",
    "    if tDictkey =='null' or tDictkey =='' or tDictkey =='[]':\r\n",
    "        valuestr = None\r\n",
    "    else:\r\n",
    "        valuestr = tDictkey\r\n",
    "    return valuestr\r\n",
    "            \r\n",
    "startTime = time.time()\r\n",
    "webFD = urllib.request.urlopen(tweetdata)\r\n",
    "\r\n",
    "newUserRows = [] # hold individual values of to-be-inserted row\r\n",
    "newGeoRows = [] # hold individual values of to-be-inserted row\r\n",
    "newTweetRows = [] # hold individual values of to-be-inserted row\r\n",
    "\r\n",
    "count = 0\r\n",
    "for i in range(500000):\r\n",
    "    if i % 5000 == 0: # Print a message every 500th tweet read\r\n",
    "        print (\"Processed \" + str(i) + \" tweets\")\r\n",
    "    tweetLine = webFD.readline() # read the file by using readline(), which reads only one line assuming there are multiple lines   \r\n",
    "    try:\r\n",
    "        if tweetLine:    # check if the item is empty before hand\r\n",
    "            # tweetLine is a byte object which needs to be decoded. \r\n",
    "            # the loads() function in the json object lets you convert the string into the json object which acts like a dictionary. \r\n",
    "            # then decode the line that come back from the web into a string. \r\n",
    "\r\n",
    "            tDict = json.loads(tweetLine.decode('utf-8'))    # using decode() and loads to convert each item to a dictionary\r\n",
    "\r\n",
    "            newUserRows.append(\r\n",
    "                (\r\n",
    "                    transformExtraneousValues(tDict['user']['id']), transformExtraneousValues(tDict['user']['name']), \r\n",
    "                    transformExtraneousValues(tDict['user']['screen_name']), transformExtraneousValues(tDict['user']['description']), \r\n",
    "                    transformExtraneousValues(tDict['user']['friends_count'])\r\n",
    "                )\r\n",
    "            )   # append the tranformed rows to the tuple then to the list\r\n",
    "            \r\n",
    "            geoV = tDict['geo']\r\n",
    "            \r\n",
    "            NoneType=type(None)\r\n",
    "        \r\n",
    "            \r\n",
    "            if geoV: # check if geoV is not null\r\n",
    "                if (not isinstance(geoV, str) or geoV.strip()) or type(geoV) is not NoneType: # Check if the key is not None neither is it a string or blanck string\r\n",
    "                    geoValue = tDict['geo']['type'] + str(tDict['geo']['coordinates'][0]) + str(tDict['geo']['coordinates'][1])\r\n",
    "            \r\n",
    "                newGeoRows.append(\r\n",
    "                    (\r\n",
    "                        tDict['geo']['type'] + str(tDict['geo']['coordinates'][0]) + str(tDict['geo']['coordinates'][1]), \r\n",
    "                        tDict['geo']['type'], tDict['geo']['coordinates'][0], tDict['geo']['coordinates'][1])\r\n",
    "\r\n",
    "                )\r\n",
    "\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(tDict['created_at']), \r\n",
    "                        transformExtraneousValues(tDict['id_str']), \r\n",
    "                        transformExtraneousValues(tDict['text']),\r\n",
    "                        transformExtraneousValues(tDict['source']),\r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(tDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(tDict['contributors']), \r\n",
    "                        transformExtraneousValues(tDict['user']['id']), \r\n",
    "                        geoValue\r\n",
    "                    )\r\n",
    "                )\r\n",
    "     \r\n",
    "                # count = count + 1\r\n",
    "            else: # for the rest of the line items where the dictionary key, 'geo' is None\r\n",
    "                # print('No result, it is none')\r\n",
    "                # pass\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(tDict['created_at']), \r\n",
    "                        transformExtraneousValues(tDict['id_str']), \r\n",
    "                        transformExtraneousValues(tDict['text']),\r\n",
    "                        transformExtraneousValues(tDict['source']),\r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(tDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(tDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(tDict['contributors']), \r\n",
    "                        transformExtraneousValues(tDict['user']['id']), \r\n",
    "                        None\r\n",
    "                    )\r\n",
    "                )\r\n",
    "    \r\n",
    "    except ValueError:\r\n",
    "        continue\r\n",
    "\r\n",
    "# execute the bulk insert queries.\r\n",
    "cur.executemany('INSERT OR IGNORE INTO UserTweets VALUES (?, ?, ?, ?, ?);',newUserRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Geo VALUES (?, ?, ?, ?);', newGeoRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Tweets VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);', newTweetRows)\r\n",
    "\r\n",
    "            \r\n",
    "userTweetItemsData = cur.execute('SELECT COUNT(*) FROM UserTweets;').fetchall()\r\n",
    "tweetItemsData = cur.execute('SELECT count(*) FROM Tweets limit 10;').fetchall()\r\n",
    "geoItemsData = cur.execute('SELECT COUNT(*) FROM Geo;').fetchall()\r\n",
    "# this is for debugging purposes\r\n",
    "# geoTweetsData = cur.execute('SELECT * FROM Tweets WHERE GeoId IS NOT NULL OR GeoId <> \"None\" Limit 10;').fetchall()\r\n",
    "\r\n",
    "\r\n",
    "print('The number of records in the userTweets table are %s'%(userTweetItemsData))\r\n",
    "print('The number of records in the Tweets table are %s' %(tweetItemsData))\r\n",
    "print('The number of records in the Geo table are %s' %(geoItemsData))\r\n",
    "# print('The Records in Tweets Object where GeoId is NOT NULL are /n %s' %(geoTweetsData))\r\n",
    "endTime = time.time()\r\n",
    "print('The processing of the tweets data took %s seconds' %(endTime-startTime))\r\n",
    "\r\n",
    "\r\n",
    "conn.commit()\r\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tweets\n",
      "Processed 5000 tweets\n",
      "Processed 10000 tweets\n",
      "Processed 15000 tweets\n",
      "Processed 20000 tweets\n",
      "Processed 25000 tweets\n",
      "Processed 30000 tweets\n",
      "Processed 35000 tweets\n",
      "Processed 40000 tweets\n",
      "Processed 45000 tweets\n",
      "Processed 50000 tweets\n",
      "Processed 55000 tweets\n",
      "Processed 60000 tweets\n",
      "Processed 65000 tweets\n",
      "Processed 70000 tweets\n",
      "Processed 75000 tweets\n",
      "Processed 80000 tweets\n",
      "Processed 85000 tweets\n",
      "Processed 90000 tweets\n",
      "Processed 95000 tweets\n",
      "Processed 100000 tweets\n",
      "Processed 105000 tweets\n",
      "Processed 110000 tweets\n",
      "Processed 115000 tweets\n",
      "Processed 120000 tweets\n",
      "Processed 125000 tweets\n",
      "Processed 130000 tweets\n",
      "Processed 135000 tweets\n",
      "Processed 140000 tweets\n",
      "Processed 145000 tweets\n",
      "Processed 150000 tweets\n",
      "Processed 155000 tweets\n",
      "Processed 160000 tweets\n",
      "Processed 165000 tweets\n",
      "Processed 170000 tweets\n",
      "Processed 175000 tweets\n",
      "Processed 180000 tweets\n",
      "Processed 185000 tweets\n",
      "Processed 190000 tweets\n",
      "Processed 195000 tweets\n",
      "Processed 200000 tweets\n",
      "Processed 205000 tweets\n",
      "Processed 210000 tweets\n",
      "Processed 215000 tweets\n",
      "Processed 220000 tweets\n",
      "Processed 225000 tweets\n",
      "Processed 230000 tweets\n",
      "Processed 235000 tweets\n",
      "Processed 240000 tweets\n",
      "Processed 245000 tweets\n",
      "Processed 250000 tweets\n",
      "Processed 255000 tweets\n",
      "Processed 260000 tweets\n",
      "Processed 265000 tweets\n",
      "Processed 270000 tweets\n",
      "Processed 275000 tweets\n",
      "Processed 280000 tweets\n",
      "Processed 285000 tweets\n",
      "Processed 290000 tweets\n",
      "Processed 295000 tweets\n",
      "Processed 300000 tweets\n",
      "Processed 305000 tweets\n",
      "Processed 310000 tweets\n",
      "Processed 315000 tweets\n",
      "Processed 320000 tweets\n",
      "Processed 325000 tweets\n",
      "Processed 330000 tweets\n",
      "Processed 335000 tweets\n",
      "Processed 340000 tweets\n",
      "Processed 345000 tweets\n",
      "Processed 350000 tweets\n",
      "Processed 355000 tweets\n",
      "Processed 360000 tweets\n",
      "Processed 365000 tweets\n",
      "Processed 370000 tweets\n",
      "Processed 375000 tweets\n",
      "Processed 380000 tweets\n",
      "Processed 385000 tweets\n",
      "Processed 390000 tweets\n",
      "Processed 395000 tweets\n",
      "Processed 400000 tweets\n",
      "Processed 405000 tweets\n",
      "Processed 410000 tweets\n",
      "Processed 415000 tweets\n",
      "Processed 420000 tweets\n",
      "Processed 425000 tweets\n",
      "Processed 430000 tweets\n",
      "Processed 435000 tweets\n",
      "Processed 440000 tweets\n",
      "Processed 445000 tweets\n",
      "Processed 450000 tweets\n",
      "Processed 455000 tweets\n",
      "Processed 460000 tweets\n",
      "Processed 465000 tweets\n",
      "Processed 470000 tweets\n",
      "Processed 475000 tweets\n",
      "Processed 480000 tweets\n",
      "Processed 485000 tweets\n",
      "Processed 490000 tweets\n",
      "Processed 495000 tweets\n",
      "The number of records in the userTweets table are [(447304,)]\n",
      "The number of records in the Tweets table are [(500000,)]\n",
      "The number of records in the Geo table are [(11849,)]\n",
      "The processing of the tweets data took 84.32156324386597 seconds\n"
     ]
    }
   ],
   "source": [
    "# Author: Ronaldlee Ejalu\r\n",
    "# Course DSC 540\r\n",
    "# Assignment module 9\r\n",
    "# reading a collection of tweets data\r\n",
    "# extending the schema by adding Geo table \r\n",
    "# also, added a foreign key relationship between Geo and Tweets. \r\n",
    "# Part 1 d\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import re\r\n",
    "import sqlite3\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import time\r\n",
    "\r\n",
    "os.chdir('C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome')\r\n",
    "\r\n",
    "# tweetdata = \"\"\"http://dbgroup.cdm.depaul.edu/DSC450/OneDayOfTweets.txt\"\"\"\r\n",
    "fileName = 'C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome/OneDayOfTweets.csv'\r\n",
    "\r\n",
    "createTbl1 = \"\"\"\r\n",
    "CREATE TABLE UserTweets \r\n",
    "(\r\n",
    "    Id VARCHAR2(100),\r\n",
    "    name VARCHAR2(100),\r\n",
    "    screen_name VARCHAR2(15),\r\n",
    "    description VARCHAR2(500),\r\n",
    "    friends_count NUMBER,\r\n",
    "    CONSTRAINT UserTweets_PK Primary Key(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl2 = \"\"\"\r\n",
    "CREATE TABLE Tweets \r\n",
    "(\tCREATED_AT DATE, \r\n",
    "\tID VARCHAR2(100), \r\n",
    "\tTEXT VARCHAR2(300), \r\n",
    "\tSOURCE VARCHAR2(100), \r\n",
    "\tIN_REPLY_TO_USER_ID VARCHAR2(12), \r\n",
    "\tIN_REPLY_TO_SCREEN_NAME VARCHAR2(15), \r\n",
    "\tIN_REPLY_TO_STATUS_ID VARCHAR2(100), \r\n",
    "\tRETWEET_COUNT NUMBER, \r\n",
    "\tCONTRIBUTORS VARCHAR2(100),\r\n",
    "    User_Id VARCHAR2(100),\r\n",
    "    GeoId VARCHAR2(1000),\r\n",
    "    CONSTRAINT TWEETS_FK1 FOREIGN KEY(User_Id) REFERENCES UserTweets(Id),\r\n",
    "    CONSTRAINT TWEETS_FK2 FOREIGN KEY(GeoId) REFERENCES Geo(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl3 = \"\"\"\r\n",
    "CREATE TABLE Geo\r\n",
    "(\r\n",
    "    Id VARCHAR2(1000),\r\n",
    "    Type VARCHAR2(50),\r\n",
    "    longitude NUMBER,\r\n",
    "    latitude NUMBER,\r\n",
    "    CONSTRAINT Geo_PK Primary Key (Id)\r\n",
    "); \r\n",
    "\"\"\"\r\n",
    "\r\n",
    "conn = sqlite3.connect('dsc450.db')                                                                        # open the connection\r\n",
    "cur = conn.cursor()                                                                                        # instantiate a cursor object\r\n",
    "\r\n",
    "# Drop the tables if they exist\r\n",
    "cur.execute('DROP TABLE IF EXISTS UserTweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Tweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Geo;')\r\n",
    "\r\n",
    "# execute the DDL to create the corresponding tables\r\n",
    "cur.execute(createTbl1)\r\n",
    "cur.execute(createTbl3)\r\n",
    "cur.execute(createTbl2)\r\n",
    "\r\n",
    "\r\n",
    "# webFD = urllib.request.urlopen(tweetdata)\r\n",
    "\r\n",
    "def transformExtraneousValues(fileDictkey):\r\n",
    "    \"\"\"A function that takes a dictionary key and \r\n",
    "    checks if the value is null, an empty string or [] \r\n",
    "    and it replaces it with None otherwise it assigns \r\n",
    "    the actual value to a variable which is returned\r\n",
    "    \"\"\"\r\n",
    "    valuestr = ''\r\n",
    "    if fileDictkey =='null' or fileDictkey =='' or fileDictkey =='[]':\r\n",
    "        valuestr = None\r\n",
    "    else:\r\n",
    "        valuestr = fileDictkey\r\n",
    "    return valuestr\r\n",
    "            \r\n",
    "\r\n",
    "def extractLine():\r\n",
    "    \"\"\"Reading the file in chunks\"\"\"\r\n",
    "    with open(fileName, 'rb') as f:\r\n",
    "        for item in f:\r\n",
    "            yield item\r\n",
    "\r\n",
    "startTime = time.time()\r\n",
    "chunkSize = 500000\r\n",
    "generatedLines = extractLine()                              # invoke a helper extractLine\r\n",
    "screenNameDict = {}\r\n",
    "fileItemsL = [i for i, j in zip(generatedLines, range(chunkSize))]\r\n",
    "\r\n",
    "newUserRows = [] # hold individual values of to-be-inserted row\r\n",
    "newGeoRows = [] # hold individual values of to-be-inserted row\r\n",
    "newTweetRows = [] # hold individual values of to-be-inserted row\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "count = 0\r\n",
    "for i in range(500000):\r\n",
    "    if i % 5000 == 0: # Print a message every 500th tweet read\r\n",
    "        print (\"Processed \" + str(i) + \" tweets\")\r\n",
    "    try:\r\n",
    "        if fileItemsL[i]:    # check if the item is empty before hand\r\n",
    "            # tweetLine is a byte object which needs to be decoded. \r\n",
    "            # the loads() function in the json object lets you convert the string into the json object which acts like a dictionary. \r\n",
    "            # then decode the line that come back from the web into a string. \r\n",
    "\r\n",
    "            fileDict = json.loads(fileItemsL[i].decode('utf-8'))    # using decode() and loads to convert each item to a dictionary\r\n",
    "\r\n",
    "            newUserRows.append(\r\n",
    "                (\r\n",
    "                    transformExtraneousValues(fileDict['user']['id']), transformExtraneousValues(fileDict['user']['name']), \r\n",
    "                    transformExtraneousValues(fileDict['user']['screen_name']), transformExtraneousValues(fileDict['user']['description']), \r\n",
    "                    transformExtraneousValues(fileDict['user']['friends_count'])\r\n",
    "                )\r\n",
    "            )   # append the tranformed rows to the tuple then to the list\r\n",
    "            \r\n",
    "            geoV = fileDict['geo']\r\n",
    "            \r\n",
    "            NoneType=type(None)\r\n",
    "        \r\n",
    "            \r\n",
    "            if geoV: # check if geoV is not null\r\n",
    "                if (not isinstance(geoV, str) or geoV.strip()) or type(geoV) is not NoneType: # Check if the key is not None neither is it a string or blanck string\r\n",
    "                    geoValue = fileDict['geo']['type'] + str(fileDict['geo']['coordinates'][0]) + str(fileDict['geo']['coordinates'][1])\r\n",
    "            \r\n",
    "                newGeoRows.append(\r\n",
    "                    (\r\n",
    "                        fileDict['geo']['type'] + str(fileDict['geo']['coordinates'][0]) + str(fileDict['geo']['coordinates'][1]), \r\n",
    "                        fileDict['geo']['type'], fileDict['geo']['coordinates'][0], fileDict['geo']['coordinates'][1])\r\n",
    "\r\n",
    "                )\r\n",
    "\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(fileDict['created_at']), \r\n",
    "                        transformExtraneousValues(fileDict['id_str']), \r\n",
    "                        transformExtraneousValues(fileDict['text']),\r\n",
    "                        transformExtraneousValues(fileDict['source']),\r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(fileDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(fileDict['contributors']), \r\n",
    "                        transformExtraneousValues(fileDict['user']['id']), \r\n",
    "                        geoValue\r\n",
    "                    )\r\n",
    "                )\r\n",
    "     \r\n",
    "                # count = count + 1\r\n",
    "            else: # for the rest of the line items where the dictionary key, 'geo' is None\r\n",
    "                # print('No result, it is none')\r\n",
    "                # pass\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(fileDict['created_at']), \r\n",
    "                        transformExtraneousValues(fileDict['id_str']), \r\n",
    "                        transformExtraneousValues(fileDict['text']),\r\n",
    "                        transformExtraneousValues(fileDict['source']),\r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(fileDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(fileDict['contributors']), \r\n",
    "                        transformExtraneousValues(fileDict['user']['id']), \r\n",
    "                        None\r\n",
    "                    )\r\n",
    "                )\r\n",
    "    \r\n",
    "    except ValueError:\r\n",
    "        continue\r\n",
    "\r\n",
    "# execute the bulk insert queries.\r\n",
    "cur.executemany('INSERT OR IGNORE INTO UserTweets VALUES (?, ?, ?, ?, ?);',newUserRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Geo VALUES (?, ?, ?, ?);', newGeoRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Tweets VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);', newTweetRows)\r\n",
    "\r\n",
    "            \r\n",
    "userTweetItemsData = cur.execute('SELECT COUNT(*) FROM UserTweets;').fetchall()\r\n",
    "tweetItemsData = cur.execute('SELECT count(*) FROM Tweets limit 10;').fetchall()\r\n",
    "geoItemsData = cur.execute('SELECT COUNT(*) FROM Geo;').fetchall()\r\n",
    "# this is for debugging purposes\r\n",
    "# geoTweetsData = cur.execute('SELECT * FROM Tweets WHERE GeoId IS NOT NULL OR GeoId <> \"None\" Limit 10;').fetchall()\r\n",
    "\r\n",
    "\r\n",
    "print('The number of records in the userTweets table are %s'%(userTweetItemsData))\r\n",
    "print('The number of records in the Tweets table are %s' %(tweetItemsData))\r\n",
    "print('The number of records in the Geo table are %s' %(geoItemsData))\r\n",
    "# print('The Records in Tweets Object where GeoId is NOT NULL are /n %s' %(geoTweetsData))\r\n",
    "endTime = time.time()\r\n",
    "print('The processing of the tweets data took %s seconds' %(endTime-startTime))\r\n",
    "\r\n",
    "\r\n",
    "conn.commit()\r\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Ronaldlee Ejalu\r\n",
    "# Course DSC 540\r\n",
    "# Assignment module 9\r\n",
    "# reading a collection of tweets data from a file\r\n",
    "# extending the schema by adding Geo table \r\n",
    "# also, added a foreign key relationship between Geo and Tweets. \r\n",
    "# Part 1 e\r\n",
    "\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import re\r\n",
    "import sqlite3\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import time\r\n",
    "\r\n",
    "os.chdir('C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome')\r\n",
    "\r\n",
    "# tweetdata = \"\"\"http://dbgroup.cdm.depaul.edu/DSC450/OneDayOfTweets.txt\"\"\"\r\n",
    "fileName = 'C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome/OneDayOfTweets.csv'\r\n",
    "\r\n",
    "createTbl1 = \"\"\"\r\n",
    "CREATE TABLE UserTweets \r\n",
    "(\r\n",
    "    Id VARCHAR2(100),\r\n",
    "    name VARCHAR2(100),\r\n",
    "    screen_name VARCHAR2(15),\r\n",
    "    description VARCHAR2(500),\r\n",
    "    friends_count NUMBER,\r\n",
    "    CONSTRAINT UserTweets_PK Primary Key(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl2 = \"\"\"\r\n",
    "CREATE TABLE Tweets \r\n",
    "(\tCREATED_AT DATE, \r\n",
    "\tID VARCHAR2(100), \r\n",
    "\tTEXT VARCHAR2(300), \r\n",
    "\tSOURCE VARCHAR2(100), \r\n",
    "\tIN_REPLY_TO_USER_ID VARCHAR2(12), \r\n",
    "\tIN_REPLY_TO_SCREEN_NAME VARCHAR2(15), \r\n",
    "\tIN_REPLY_TO_STATUS_ID VARCHAR2(100), \r\n",
    "\tRETWEET_COUNT NUMBER, \r\n",
    "\tCONTRIBUTORS VARCHAR2(100),\r\n",
    "    User_Id VARCHAR2(100),\r\n",
    "    GeoId VARCHAR2(1000),\r\n",
    "    CONSTRAINT TWEETS_FK1 FOREIGN KEY(User_Id) REFERENCES UserTweets(Id),\r\n",
    "    CONSTRAINT TWEETS_FK2 FOREIGN KEY(GeoId) REFERENCES Geo(Id)\r\n",
    ");\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "createTbl3 = \"\"\"\r\n",
    "CREATE TABLE Geo\r\n",
    "(\r\n",
    "    Id VARCHAR2(1000),\r\n",
    "    Type VARCHAR2(50),\r\n",
    "    longitude NUMBER,\r\n",
    "    latitude NUMBER,\r\n",
    "    CONSTRAINT Geo_PK Primary Key (Id)\r\n",
    "); \r\n",
    "\"\"\"\r\n",
    "\r\n",
    "conn = sqlite3.connect('dsc450.db')                                                                        # open the connection\r\n",
    "cur = conn.cursor()                                                                                        # instantiate a cursor object\r\n",
    "\r\n",
    "# Drop the tables if they exist\r\n",
    "cur.execute('DROP TABLE IF EXISTS UserTweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Tweets;')\r\n",
    "cur.execute('DROP TABLE IF EXISTS Geo;')\r\n",
    "\r\n",
    "# execute the DDL to create the corresponding tables\r\n",
    "cur.execute(createTbl1)\r\n",
    "cur.execute(createTbl3)\r\n",
    "cur.execute(createTbl2)\r\n",
    "\r\n",
    "\r\n",
    "def transformExtraneousValues(fileDictkey):\r\n",
    "    \"\"\"A function that takes a dictionary key and \r\n",
    "    checks if the value is null, an empty string or [] \r\n",
    "    and it replaces it with None otherwise it assigns \r\n",
    "    the actual value to a variable which is returned\r\n",
    "    \"\"\"\r\n",
    "   \r\n",
    "    valuestr = ''\r\n",
    "    if fileDictkey =='null' or fileDictkey =='' or fileDictkey =='[]':\r\n",
    "        valuestr = None\r\n",
    "    else:\r\n",
    "        valuestr = fileDictkey\r\n",
    "    return valuestr\r\n",
    "            \r\n",
    "\r\n",
    "def extractLine():\r\n",
    "    \"\"\"Reading the file in chunks\"\"\"\r\n",
    "    with open(fileName, 'rb') as f:\r\n",
    "        for item in f:\r\n",
    "            yield item\r\n",
    "\r\n",
    "startTime = time.time()\r\n",
    "chunkSize = 500000\r\n",
    "generatedLines = extractLine()                              # invoke a helper extractLine to read  the file in chunks\r\n",
    "screenNameDict = {}\r\n",
    "fileItemsL = [i for i, j in zip(generatedLines, range(chunkSize))]\r\n",
    "\r\n",
    "newUserRows = [] # hold individual values of to-be-inserted row\r\n",
    "newGeoRows = [] # hold individual values of to-be-inserted row\r\n",
    "newTweetRows = [] # hold individual values of to-be-inserted row\r\n",
    "\r\n",
    "batchUserRows = [] # hold the batch size of rows to be inserted\r\n",
    "batchGeoRows = []\r\n",
    "batchTweetRows = []\r\n",
    "\r\n",
    "tweetCounter = 0\r\n",
    "geoCounter = 0\r\n",
    "userCounter = 0\r\n",
    "for i in range(500000):\r\n",
    "    if i % 2000 == 0: # Print a message every 500th tweet read\r\n",
    "        print (\"Processed \" + str(i) + \" tweets\")\r\n",
    "    try:\r\n",
    "        if fileItemsL[i]:    # check if the item is empty before hand\r\n",
    "            # tweetLine is a byte object which needs to be decoded. \r\n",
    "            # the loads() function in the json object lets you convert the string into the json object which acts like a dictionary. \r\n",
    "            # then decode the line that come back from the web into a string. \r\n",
    "\r\n",
    "            fileDict = json.loads(fileItemsL[i].decode('utf-8'))    # using decode() and loads to convert each item to a dictionary\r\n",
    "            \r\n",
    "            newUserRows.append(\r\n",
    "                (\r\n",
    "                    transformExtraneousValues(fileDict['user']['id']), transformExtraneousValues(fileDict['user']['name']), \r\n",
    "                    transformExtraneousValues(fileDict['user']['screen_name']), transformExtraneousValues(fileDict['user']['description']), \r\n",
    "                    transformExtraneousValues(fileDict['user']['friends_count'])\r\n",
    "                )\r\n",
    "            )   # append the tranformed rows to the tuple then to the list\r\n",
    "            userCounter += 1\r\n",
    "            \r\n",
    "            geoV = fileDict['geo']\r\n",
    "            \r\n",
    "            NoneType=type(None)\r\n",
    "        \r\n",
    "            \r\n",
    "            if geoV: # check if geoV is not null\r\n",
    "                if (not isinstance(geoV, str) or geoV.strip()) or type(geoV) is not NoneType: # Check if the key is not None neither is it a string or blanck string\r\n",
    "                    geoValue = fileDict['geo']['type'] + str(fileDict['geo']['coordinates'][0]) + str(fileDict['geo']['coordinates'][1])\r\n",
    "            \r\n",
    "                newGeoRows.append(\r\n",
    "                    (\r\n",
    "                        fileDict['geo']['type'] + str(fileDict['geo']['coordinates'][0]) + str(fileDict['geo']['coordinates'][1]), \r\n",
    "                        fileDict['geo']['type'], fileDict['geo']['coordinates'][0], fileDict['geo']['coordinates'][1])\r\n",
    "\r\n",
    "                )\r\n",
    "\r\n",
    "                geoCounter += 1 \r\n",
    "\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(fileDict['created_at']), \r\n",
    "                        transformExtraneousValues(fileDict['id_str']), \r\n",
    "                        transformExtraneousValues(fileDict['text']),\r\n",
    "                        transformExtraneousValues(fileDict['source']),\r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(fileDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(fileDict['contributors']), \r\n",
    "                        transformExtraneousValues(fileDict['user']['id']), \r\n",
    "                        geoValue\r\n",
    "                    )\r\n",
    "                )\r\n",
    "                tweetCounter += 1\r\n",
    "     \r\n",
    "                \r\n",
    "            else: # for the rest of the line items where the dictionary key, 'geo' is None\r\n",
    "                # print('No result, it is none')\r\n",
    "                # pass\r\n",
    "                newTweetRows.append(\r\n",
    "                    (\r\n",
    "                        transformExtraneousValues(fileDict['created_at']), \r\n",
    "                        transformExtraneousValues(fileDict['id_str']), \r\n",
    "                        transformExtraneousValues(fileDict['text']),\r\n",
    "                        transformExtraneousValues(fileDict['source']),\r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_user_id']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_screen_name']), \r\n",
    "                        transformExtraneousValues(fileDict['in_reply_to_status_id']), \r\n",
    "                        transformExtraneousValues(fileDict['retweet_count']), \r\n",
    "                        transformExtraneousValues(fileDict['contributors']), \r\n",
    "                        transformExtraneousValues(fileDict['user']['id']), \r\n",
    "                        None\r\n",
    "                    )\r\n",
    "                )\r\n",
    "                tweetCounter += 1\r\n",
    "    \r\n",
    "    except ValueError:\r\n",
    "        continue\r\n",
    "\r\n",
    "    # Using a batch size of 2000 to peform bulk inserts \r\n",
    "    if userCounter > 2000:\r\n",
    "        cur.executemany('INSERT OR IGNORE INTO UserTweets VALUES (?, ?, ?, ?, ?);',newUserRows)\r\n",
    "        # rows = len(newUserRows)        # enable this for debugging purposes\r\n",
    "        newUserRows = []      # cleaning up the list\r\n",
    "        userCounter = 0         # reset the counter\r\n",
    "        # print('Inserted %s rows' %(rows))\r\n",
    "    \r\n",
    "    if geoCounter > 2000:\r\n",
    "        cur.executemany('INSERT OR IGNORE INTO Geo VALUES (?, ?, ?, ?);', newGeoRows)\r\n",
    "        newGeoRows = []     # cleaning up the list before the next tuples of data are added\r\n",
    "        geoCounter = 0\r\n",
    "\r\n",
    "    if tweetCounter > 2000:\r\n",
    "        cur.executemany('INSERT OR IGNORE INTO Tweets VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);', newTweetRows)\r\n",
    "        newTweetRows = []\r\n",
    "        tweetCounter = 0\r\n",
    "# execute the remaining data the list when the batch size is less than 2000\r\n",
    "cur.executemany('INSERT OR IGNORE INTO UserTweets VALUES (?, ?, ?, ?, ?);',newUserRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Geo VALUES (?, ?, ?, ?);', newGeoRows)\r\n",
    "cur.executemany('INSERT OR IGNORE INTO Tweets VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);', newTweetRows)\r\n",
    "\r\n",
    "userTweetItemsData = cur.execute('SELECT COUNT(*) FROM UserTweets;').fetchall()\r\n",
    "tweetItemsData = cur.execute('SELECT count(*) FROM Tweets limit 10;').fetchall()\r\n",
    "geoItemsData = cur.execute('SELECT COUNT(*) FROM Geo;').fetchall()\r\n",
    "# this is for debugging purposes\r\n",
    "# geoTweetsData = cur.execute('SELECT * FROM Tweets WHERE GeoId IS NOT NULL OR GeoId <> \"None\" Limit 10;').fetchall()\r\n",
    "\r\n",
    "\r\n",
    "print('The number of records in the userTweets table are %s'%(userTweetItemsData))\r\n",
    "print('The number of records in the Tweets table are %s' %(tweetItemsData))\r\n",
    "print('The number of records in the Geo table are %s' %(geoItemsData))\r\n",
    "# print('The Records in Tweets Object where GeoId is NOT NULL are /n %s' %(geoTweetsData))\r\n",
    "endTime = time.time()\r\n",
    "print('The processing of the tweets data took %s seconds' %(endTime-startTime))\r\n",
    "\r\n",
    "\r\n",
    "conn.commit()\r\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Ronaldlee Ejalu\r\n",
    "# Course DSC 540\r\n",
    "# Final Exam \r\n",
    "# a SQL query that finds the smallest \r\n",
    "# longitude and latitude value for each user ID\r\n",
    "# Part 2a\r\n",
    "import re\r\n",
    "import sqlite3\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "os.chdir('C:/Users/rejalu1/OneDrive - Henry Ford Health System/DSC450/Assignments/FinalExamTakeHome')\r\n",
    "# connect to the database file to open a connection\r\n",
    "conn = sqlite3.connect('dsc450.db')                                  # open the connection\r\n",
    "cur = conn.cursor()                                                    # instantiate a cursor object\r\n",
    "# construct a sql query and assign it to a string variable\r\n",
    "sqlScript = \"\"\" \r\n",
    "SELECT Tweets.User_Id, \r\n",
    "MIN(Geo.longitude), \r\n",
    "MIN(latitude) \r\n",
    "FROM Tweets, Geo \r\n",
    "WHERE Tweets.GeoId = Geo.Id \r\n",
    "GROUP BY Tweets.User_Id; \r\n",
    "\"\"\"\r\n",
    "userIdRes = cur.execute(sqlScript).fetchall()\r\n",
    "print('The results of query %s are: /n %s ' %(sqlScript, userIdRes))\r\n",
    "print('%s rows are returned' %(len(userIdRes)))\r\n",
    "cur.close()\r\n",
    "\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83aae834e84db1c94a40c7928f312c06468b9b5c4a49ea351ffd31dad3e197f9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cmdpy37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}